#!/usr/bin/env python3
"""
Social Bladeæ•°æ®æ”¶é›†å·¥å…·
ä¸“é—¨ç”¨äºè·å–YesWelderåŠå…¶ç«äº‰å¯¹æ‰‹çš„YouTubeç»Ÿè®¡æ•°æ®
"""

import asyncio
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse

from playwright.async_api import async_playwright, Page, Browser, BrowserContext
import pandas as pd


class SocialBladeDataCollector:
    """Social Bladeä¸“ç”¨æ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        self.base_url = "https://socialblade.com"
        self.output_dir = Path("socialblade_data")
        self.output_dir.mkdir(exist_ok=True)

        # ç›®æ ‡é¢‘é“åˆ—è¡¨
        self.target_channels = {
            'yeswelder': {
                'url': '/youtube/handle/yeswelder',
                'name': 'YesWelder',
                'category': 'Target'
            },
            'lincoln_electric': {
                'url': '/youtube/user/lincolnelectric',
                'name': 'Lincoln Electric',
                'category': 'Competitor'
            },
            'hobart_welding': {
                'url': '/youtube/user/hobartwelding',
                'name': 'Hobart Welding',
                'category': 'Competitor'
            },
            'miller_electric': {
                'url': '/youtube/user/millerwelders',
                'name': 'Miller Electric',
                'category': 'Competitor'
            },
            'weldpro': {
                'url': '/youtube/handle/weldpro',
                'name': 'WeldPro',
                'category': 'Competitor'
            }
        }

        # åæ£€æµ‹é…ç½®
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

    async def setup_browser(self, playwright) -> Tuple[Browser, BrowserContext]:
        """è®¾ç½®åæ£€æµ‹æµè§ˆå™¨"""
        browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox'
            ]
        )

        context = await browser.new_context(
            user_agent=self.user_agents[0],
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers={
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Referer': 'https://www.google.com/',
            },
            java_script_enabled=True
        )

        # ç»•è¿‡webdriveræ£€æµ‹
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });

            Object.defineProperty(navigator, 'plugins', {
                get: () => [
                    {
                        0: {type: "application/x-google-chrome-pdf"},
                        description: "Portable Document Format",
                        filename: "internal-pdf-viewer",
                        length: 1,
                        name: "Chrome PDF Plugin"
                    }
                ],
            });

            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en'],
            });
        """)

        return browser, context

    async def wait_for_content(self, page: Page, timeout: int = 30000) -> bool:
        """ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…ä¸»è¦å†…å®¹åŒºåŸŸ
            await page.wait_for_selector('.user-info, .YouTubeUserTopInfo, .content, #main-content', timeout=timeout)

            # ç­‰å¾…åŠ è½½åŠ¨ç”»æ¶ˆå¤±
            await page.wait_for_timeout(2000)

            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
            content = await page.evaluate("""() => {
                const mainContent = document.querySelector('.user-info, .YouTubeUserTopInfo, .content, #main-content');
                return mainContent && mainContent.innerText.length > 100;
            }""")

            if not content:
                print("âš ï¸ é¡µé¢å†…å®¹åŠ è½½ä¸å®Œæ•´ï¼Œç»§ç»­ç­‰å¾…...")
                await page.wait_for_timeout(3000)

            return True

        except Exception as e:
            print(f"âš ï¸ ç­‰å¾…å†…å®¹åŠ è½½è¶…æ—¶: {e}")
            return False

    async def extract_socialblade_data(self, page: Page, channel_name: str) -> Dict[str, Any]:
        """æå–Social Bladeæ•°æ®"""
        try:
            # ç­‰å¾…æ•°æ®åŠ è½½
            await self.wait_for_content(page)

            # æå–åŸºæœ¬ä¿¡æ¯
            basic_info = await page.evaluate("""() => {
                const data = {};

                // æå–é¢‘é“åç§°
                const nameElement = document.querySelector('h1, .user-name, .channel-name, .YouTubeUserTopInfo h1');
                data.channel_name = nameElement ? nameElement.innerText.trim() : 'Unknown';

                // æå–è®¢é˜…è€…æ•°é‡
                const subscriberElements = document.querySelectorAll('.subscribers, .youtube-subscribers, [data-stat="subscribers"]');
                subscriberElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /subscribers?/i.test(text)) {
                        data.subscribers = text;
                    }
                });

                // æå–è§‚çœ‹æ¬¡æ•°
                const viewElements = document.querySelectorAll('.views, .youtube-views, [data-stat="views"]');
                viewElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /views?/i.test(text)) {
                        data.views = text;
                    }
                });

                // æå–è§†é¢‘æ•°é‡
                const videoElements = document.querySelectorAll('.videos, .youtube-videos, [data-stat="videos"]');
                videoElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /videos?/i.test(text)) {
                        data.videos = text;
                    }
                });

                // æå–å›½å®¶/åœ°åŒº
                const countryElement = document.querySelector('.country, .location, .user-country');
                data.country = countryElement ? countryElement.innerText.trim() : 'Unknown';

                // æå–é¢‘é“ç±»å‹
                const typeElement = document.querySelector('.channel-type, .user-type');
                data.channel_type = typeElement ? typeElement.innerText.trim() : 'Unknown';

                // æå–åŠ å…¥æ—¥æœŸ
                const joinedElement = document.querySelector('.joined, .user-date, .channel-created');
                data.joined_date = joinedElement ? joinedElement.innerText.trim() : 'Unknown';

                return data;
            }""")

            # æå–ç»Ÿè®¡æ•°æ®
            stats_data = await page.evaluate("""() => {
                const stats = {};

                // æŸ¥æ‰¾æ‰€æœ‰ç»Ÿè®¡æ•°æ®
                const statElements = document.querySelectorAll('.stat, .stat-item, .data-point, .metric');
                statElements.forEach(el => {
                    const label = el.querySelector('.label, .stat-label, .data-label') || el;
                    const value = el.querySelector('.value, .stat-value, .data-value') || el;

                    const labelText = label.innerText.trim();
                    const valueText = value.innerText.trim();

                    if (labelText && valueText && labelText !== valueText) {
                        stats[labelText] = valueText;
                    }
                });

                return stats;
            }""")

            # æå–å¢é•¿ç‡æ•°æ®
            growth_data = await page.evaluate("""() => {
                const growth = {};

                // æŸ¥æ‰¾å¢é•¿ç‡ç›¸å…³æ•°æ®
                const growthElements = document.querySelectorAll('.growth, .growth-rate, .change, .trend');
                growthElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /%|\+|\-|growth|change|trend/i.test(text)) {
                        growth[text] = text;
                    }
                });

                return growth;
            }""")

            # æå–ä¼°å€¼å’Œæ”¶å…¥æ•°æ®
            financial_data = await page.evaluate("""() => {
                const financial = {};

                // æŸ¥æ‰¾ä¼°å€¼å’Œæ”¶å…¥ç›¸å…³æ•°æ®
                const financialElements = document.querySelectorAll('.estimated, .valuation, .revenue, .income, .earnings');
                financialElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /(\$|â‚¬|Â£|Â¥|estimated|valuation|revenue|income|earnings)/i.test(text)) {
                        financial[text] = text;
                    }
                });

                return financial;
            }""")

            # æå–æ’åæ•°æ®
            ranking_data = await page.evaluate("""() => {
                const ranking = {};

                // æŸ¥æ‰¾æ’åç›¸å…³æ•°æ®
                const rankingElements = document.querySelectorAll('.rank, .ranking, .position, .ranked');
                rankingElements.forEach(el => {
                    const text = el.innerText.trim();
                    if (text && /rank|position|#/i.test(text)) {
                        ranking[text] = text;
                    }
                });

                return ranking;
            }""")

            # æå–å›¾è¡¨æ•°æ®
            charts_data = await page.evaluate("""() => {
                const charts = {};

                // æŸ¥æ‰¾å›¾è¡¨ç›¸å…³çš„æ•°æ®
                const chartElements = document.querySelectorAll('.chart, .graph, .stats-chart, .growth-chart');
                chartElements.forEach((el, index) => {
                    charts[`chart_${index}`] = {
                        className: el.className,
                        innerText: el.innerText,
                        dataAttributes: {}
                    };

                    // æå–æ•°æ®å±æ€§
                    el.getAttributeNames().forEach(attr => {
                        if (attr.startsWith('data-')) {
                            charts[`chart_${index}`].dataAttributes[attr] = el.getAttribute(attr);
                        }
                    });
                });

                return charts;
            }""")

            # ç»„åˆæ‰€æœ‰æ•°æ®
            extracted_data = {
                'channel_name': channel_name,
                'basic_info': basic_info,
                'stats': stats_data,
                'growth': growth_data,
                'financial': financial_data,
                'ranking': ranking_data,
                'charts': charts_data,
                'url': page.url,
                'scraped_at': datetime.now().isoformat()
            }

            return extracted_data

        except Exception as e:
            print(f"âŒ æå–{channel_name}æ•°æ®æ—¶å‡ºé”™: {e}")
            return {}

    async def capture_screenshot(self, page: Page, filename: str) -> Optional[str]:
        """æˆªå–é¡µé¢æˆªå›¾"""
        try:
            screenshot_path = self.output_dir / f"{filename}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            print(f"ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            return str(screenshot_path)
        except Exception as e:
            print(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            return None

    async def scrape_channel(self, channel_key: str, channel_info: Dict[str, str]) -> Dict[str, Any]:
        """çˆ¬å–å•ä¸ªé¢‘é“æ•°æ®"""
        print(f"ğŸ” å¼€å§‹çˆ¬å– {channel_info['name']} ({channel_key})...")

        async with async_playwright() as playwright:
            browser, context = await self.setup_browser(playwright)

            try:
                page = await context.new_page()
                page.set_default_timeout(30000)

                # æ„å»ºURL
                url = urljoin(self.base_url, channel_info['url'])
                print(f"ğŸ“ è®¿é—®: {url}")

                # è®¿é—®é¡µé¢
                await page.goto(url, wait_until="networkidle")

                # ç­‰å¾…å†…å®¹åŠ è½½
                print("â³ ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½...")
                if not await self.wait_for_content(page):
                    print("âš ï¸ é¡µé¢å¯èƒ½åŠ è½½ä¸å®Œå…¨ï¼Œç»§ç»­å°è¯•æå–æ•°æ®...")

                # æå–æ•°æ®
                print("ğŸ“Š æå–Social Bladeæ•°æ®...")
                channel_data = await self.extract_socialblade_data(page, channel_info['name'])

                # æˆªå›¾
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                screenshot_path = await self.capture_screenshot(page, f"{channel_key}_{timestamp}")

                if screenshot_path:
                    channel_data['screenshot'] = screenshot_path

                # æ·»åŠ å…ƒæ•°æ®
                channel_data['channel_key'] = channel_key
                channel_data['channel_category'] = channel_info['category']
                channel_data['target_url'] = url

                return channel_data

            except Exception as e:
                print(f"âŒ çˆ¬å–{channel_info['name']}æ—¶å‡ºé”™: {e}")
                return {}
            finally:
                await browser.close()

    def parse_numeric_value(self, value_str: str) -> float:
        """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼ˆå¦‚ 1.2M, 345K, 1,234ï¼‰"""
        if not value_str:
            return 0

        # ç§»é™¤é€—å·å’Œç©ºæ ¼
        clean_str = value_str.replace(',', '').strip()

        # åŒ¹é…æ•°å­—éƒ¨åˆ†
        match = re.match(r'^([0-9.]+)([KMB]?)$', clean_str.upper())
        if not match:
            return 0

        number, suffix = match.groups()
        num_value = float(number)

        # å¤„ç†åç¼€
        if suffix == 'K':
            num_value *= 1000
        elif suffix == 'M':
            num_value *= 1000000
        elif suffix == 'B':
            num_value *= 1000000000

        return num_value

    def process_channel_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†åŸå§‹æ•°æ®ï¼Œæå–å…³é”®æŒ‡æ ‡"""
        if not raw_data:
            return {}

        processed = {
            'channel_key': raw_data.get('channel_key'),
            'channel_name': raw_data.get('channel_name'),
            'channel_category': raw_data.get('channel_category'),
            'scraped_at': raw_data.get('scraped_at'),
            'url': raw_data.get('url'),
            'target_url': raw_data.get('target_url')
        }

        # å¤„ç†åŸºæœ¬ä¿¡æ¯
        basic_info = raw_data.get('basic_info', {})
        processed['basic_info'] = basic_info

        # è§£æè®¢é˜…è€…æ•°é‡
        subscribers = basic_info.get('subscribers', '')
        processed['subscribers_raw'] = subscribers
        processed['subscribers_count'] = self.parse_numeric_value(subscribers)

        # è§£æè§‚çœ‹æ¬¡æ•°
        views = basic_info.get('views', '')
        processed['views_raw'] = views
        processed['views_count'] = self.parse_numeric_value(views)

        # è§£æè§†é¢‘æ•°é‡
        videos = basic_info.get('videos', '')
        processed['videos_raw'] = videos
        processed['videos_count'] = self.parse_numeric_value(videos)

        # å¤„ç†ç»Ÿè®¡æ•°æ®
        stats = raw_data.get('stats', {})
        processed['stats'] = stats

        # å¤„ç†å¢é•¿ç‡æ•°æ®
        growth = raw_data.get('growth', {})
        processed['growth'] = growth

        # å¤„ç†è´¢åŠ¡æ•°æ®
        financial = raw_data.get('financial', {})
        processed['financial'] = financial

        # å¤„ç†æ’åæ•°æ®
        ranking = raw_data.get('ranking', {})
        processed['ranking'] = ranking

        # å¤„ç†å›¾è¡¨æ•°æ®
        charts = raw_data.get('charts', {})
        processed['charts'] = charts

        return processed

    def save_channel_data(self, channel_data: Dict[str, Any]) -> str:
        """ä¿å­˜é¢‘é“æ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        channel_key = channel_data.get('channel_key', 'unknown')

        # ä¿å­˜JSONæ ¼å¼
        json_file = self.output_dir / f"{channel_key}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(channel_data, f, indent=2, ensure_ascii=False)

        # ä¿å­˜å¤„ç†åçš„CSVæ ¼å¼
        processed_data = self.process_channel_data(channel_data)
        if processed_data:
            csv_file = self.output_dir / f"{channel_key}_processed_{timestamp}.csv"

            # åˆ›å»ºDataFrame
            df_data = {
                'channel_key': [processed_data.get('channel_key')],
                'channel_name': [processed_data.get('channel_name')],
                'channel_category': [processed_data.get('channel_category')],
                'subscribers_count': [processed_data.get('subscribers_count')],
                'subscribers_raw': [processed_data.get('subscribers_raw')],
                'views_count': [processed_data.get('views_count')],
                'views_raw': [processed_data.get('views_raw')],
                'videos_count': [processed_data.get('videos_count')],
                'videos_raw': [processed_data.get('videos_raw')],
                'scraped_at': [processed_data.get('scraped_at')],
                'url': [processed_data.get('url')]
            }

            df = pd.DataFrame(df_data)
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"ğŸ“Š CSVæ•°æ®å·²ä¿å­˜: {csv_file}")

        print(f"âœ… {channel_key}æ•°æ®å·²ä¿å­˜: {json_file}")
        return str(json_file)

    async def scrape_all_channels(self) -> Dict[str, Any]:
        """çˆ¬å–æ‰€æœ‰ç›®æ ‡é¢‘é“"""
        print("ğŸš€ å¼€å§‹Social Bladeæ•°æ®æ”¶é›†...")
        print("=" * 60)

        all_data = {}
        successful_channels = []
        failed_channels = []

        for channel_key, channel_info in self.target_channels.items():
            try:
                print(f"\nğŸ“¡ å¤„ç†é¢‘é“: {channel_info['name']}")
                print("-" * 40)

                # çˆ¬å–é¢‘é“æ•°æ®
                channel_data = await self.scrape_channel(channel_key, channel_info)

                if channel_data:
                    # ä¿å­˜æ•°æ®
                    saved_file = self.save_channel_data(channel_data)
                    all_data[channel_key] = channel_data
                    successful_channels.append(channel_key)
                    print(f"âœ… {channel_info['name']} æ•°æ®æ”¶é›†æˆåŠŸ")
                else:
                    failed_channels.append(channel_key)
                    print(f"âŒ {channel_info['name']} æ•°æ®æ”¶é›†å¤±è´¥")

                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¢«å°
                await asyncio.sleep(2)

            except Exception as e:
                failed_channels.append(channel_key)
                print(f"âŒ {channel_info['name']} å¤„ç†å‡ºé”™: {e}")

        # ä¿å­˜æ±‡æ€»æ•°æ®
        summary = {
            'scraped_at': datetime.now().isoformat(),
            'total_channels': len(self.target_channels),
            'successful_channels': successful_channels,
            'failed_channels': failed_channels,
            'success_rate': len(successful_channels) / len(self.target_channels) * 100,
            'channels_data': all_data
        }

        summary_file = self.output_dir / f"socialblade_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“‹ æ±‡æ€»æ•°æ®å·²ä¿å­˜: {summary_file}")

        # æ‰“å°æ‘˜è¦
        print("\nğŸ“Š æ•°æ®æ”¶é›†æ‘˜è¦:")
        print(f"   æ€»é¢‘é“æ•°: {summary['total_channels']}")
        print(f"   æˆåŠŸæ”¶é›†: {len(successful_channels)}")
        print(f"   å¤±è´¥é¢‘é“: {len(failed_channels)}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")

        if failed_channels:
            print(f"   å¤±è´¥é¢‘é“: {', '.join(failed_channels)}")

        return summary

    def generate_competitive_analysis(self, summary_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç«å“åˆ†ææŠ¥å‘Š"""
        channels_data = summary_data.get('channels_data', {})

        if not channels_data:
            return "æ— å¯ç”¨æ•°æ®è¿›è¡Œç«å“åˆ†æ"

        # å¤„ç†æ•°æ®ä»¥ä¾¿åˆ†æ
        processed_channels = {}
        for channel_key, raw_data in channels_data.items():
            processed = self.process_channel_data(raw_data)
            if processed:
                processed_channels[channel_key] = processed

        if not processed_channels:
            return "æ— æœ‰æ•ˆæ•°æ®è¿›è¡Œç«å“åˆ†æ"

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = []
        report.append("# Social Bladeç«å“åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # åŸºæœ¬ç»Ÿè®¡
        report.append("## åŸºæœ¬ç»Ÿè®¡")
        report.append("")

        for channel_key, data in processed_channels.items():
            report.append(f"### {data.get('channel_name', channel_key)}")
            report.append(f"- è®¢é˜…è€…: {data.get('subscribers_raw', 'N/A')} ({data.get('subscribers_count', 0):,})")
            report.append(f"- è§‚çœ‹æ¬¡æ•°: {data.get('views_raw', 'N/A')} ({data.get('views_count', 0):,})")
            report.append(f"- è§†é¢‘æ•°é‡: {data.get('videos_raw', 'N/A')} ({data.get('videos_count', 0):,})")
            report.append("")

        # æ’ååˆ†æ
        report.append("## æ’ååˆ†æ")
        report.append("")

        # æŒ‰è®¢é˜…è€…æ’åº
        sorted_by_subscribers = sorted(processed_channels.items(), key=lambda x: x[1].get('subscribers_count', 0), reverse=True)

        report.append("### è®¢é˜…è€…æ’å")
        for i, (channel_key, data) in enumerate(sorted_by_subscribers, 1):
            report.append(f"{i}. {data.get('channel_name', channel_key)}: {data.get('subscribers_raw', 'N/A')}")
        report.append("")

        # æŒ‰è§‚çœ‹æ¬¡æ•°æ’åº
        sorted_by_views = sorted(processed_channels.items(), key=lambda x: x[1].get('views_count', 0), reverse=True)

        report.append("### è§‚çœ‹æ¬¡æ•°æ’å")
        for i, (channel_key, data) in enumerate(sorted_by_views, 1):
            report.append(f"{i}. {data.get('channel_name', channel_key)}: {data.get('views_raw', 'N/A')}")
        report.append("")

        # å¯¹æ¯”åˆ†æ
        if len(processed_channels) > 1:
            report.append("## å¯¹æ¯”åˆ†æ")
            report.append("")

            # æ‰¾åˆ°ç›®æ ‡é¢‘é“
            target_channel = processed_channels.get('yeswelder')
            if target_channel:
                report.append("### YesWelder vs ç«äº‰å¯¹æ‰‹")
                report.append("")

                target_subs = target_channel.get('subscribers_count', 0)
                target_views = target_channel.get('views_count', 0)

                for channel_key, data in processed_channels.items():
                    if channel_key != 'yeswelder':
                        subs_ratio = (data.get('subscribers_count', 0) / target_subs * 100) if target_subs > 0 else 0
                        views_ratio = (data.get('views_count', 0) / target_views * 100) if target_views > 0 else 0

                        report.append(f"#### {data.get('channel_name', channel_key)}")
                        report.append(f"- è®¢é˜…è€…æ¯”ä¾‹: {subs_ratio:.1f}%")
                        report.append(f"- è§‚çœ‹æ¬¡æ•°æ¯”ä¾‹: {views_ratio:.1f}%")
                        report.append("")

        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        report_file = self.output_dir / f"competitive_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“‹ ç«å“åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        return report_content

    async def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ•°æ®æ”¶é›†å™¨"""
        print("ğŸ¯ Social Bladeæ•°æ®æ”¶é›†å™¨å¯åŠ¨")
        print("=" * 60)

        # æ‰§è¡Œæ•°æ®æ”¶é›†
        summary_data = await self.scrape_all_channels()

        if summary_data.get('channels_data'):
            # ç”Ÿæˆç«å“åˆ†ææŠ¥å‘Š
            analysis_report = self.generate_competitive_analysis(summary_data)
            summary_data['analysis_report'] = analysis_report

            print("\nğŸ‰ Social Bladeæ•°æ®æ”¶é›†å®Œæˆï¼")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶ä½ç½®: {self.output_dir}")
        else:
            print("âŒ æ•°æ®æ”¶é›†å¤±è´¥")

        return summary_data


async def main():
    """ä¸»å‡½æ•°"""
    collector = SocialBladeDataCollector()
    await collector.run()


if __name__ == "__main__":
    asyncio.run(main())